---
title: "Data Science using a Data Lake"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float:
      collapsed: no
---

## Introduction

We noticed that the types of questions we would field after a demo of sparklyr to our customers were more about high level architecture than how the package works.  So we put together some slides to illustrate and talk about important concepts that helped customers see where Spark and R and sparklyr fits in a Big Data Platform implementation.

In this article, we'll review those slides and provide a narrative that will hopefully help you better envision how you can take advantage of our products.


## R for Data Science

It is very important to preface the Use Case review with where RStudio focuses its efforts when developing packages and products.  Many vendors offer *R integration*, but in most cases what it means is that they will add a model built in R to their pipeline or interface, pass new inputs to that model in order to get outputs that can be used in next step in the pipeline, or use in a calculation for the interface.

Our focus is on the process that happens before, the discipline that produces the model, meaning **Data Science**.  

<center><p><img src="images/slide-1.png" align='center'/></p></center>


In their [R for Data Science](http://r4ds.had.co.nz/) book, Hadley Wickham and Garret Grolemund provide a great diagram that nicely illustrates the the Data Science program: We **import** data into memory with R and clean and **tidy** the data.  Then we go into a cyclical process call **understand**, which helps us to get to know our data, and hopefully find the answer to the question we started with.  This cycle typically involves making **transformations** to our tidied data, use the transformed data to fit **models** and **visualize** results. Once we find an answer to our question we then **communicate** the results.

Data Scientists like using R because it allows them to complete a Data Science project from beginning to end inside the R environment and in memory.  


## Hadoop as a Data Source

What happens when the data that needs to be analyzed is very large, like the ones found in a Hadoop cluster?  It would be impossible to fit in memory, so workarounds are normally used.  These workarounds include: using a comparatively minuscule data sample or download as much data as possible.  This becomes disruptive to Data Scientists because either the small sample may not be representative, or they have wait a long time in every iteration of importing a lot of data, exploring a lot of data and modeling a lot of data.  

<center><p><img src="images/slide-2.png" align='center'/></p></center>

## Spark as an Analysis Engine

We noticed that a very important mental leap to make is to see Spark not just as a gateway to Hadoop,  or worse, an additional data source.  Spark is a computing engine, so as such, we should think of it as an excellent vehicle to scale our analytics.  Spark has many capabilities that makes it ideal for Data Science in a data lake, like close integration with Hadoop and Hive, ability to cache data into memory across multiple nodes, data transformers, and its Machine Learning libraries.  

The approach then is to **push as much compute** to the cluster, so R will mostly be used as an interface for the Data Scientist to Spark, who will then **collect as little results** as possible back into R memory, mostly to **visualize** and **communicate**.  As shown in the slide, the more **import**, **tidy**, **transform** and **modeling** work we can push to Spark, the faster we can analyze very large data sets.

<center><p><img src="images/slide-3.png" align='center'/></p></center>

## Cluster Setup

Here is an illustration of how R, RStudio and sparklyr can be added to the YARN managed cluster.  The highlights are:

- R, RStudio and sparklyr need to be installed in one node only, typically an edge node
- The Data Scientist can access R, Spark and the cluster via a web browser by navigating to the RStudio IDE inside the edge node

<center><p><img src="images/slide-4.png" align='center'/></p></center>

## Considerations 

There are some important considerations to keep in mind when deciding to combine your Data Lake and R for large scale analytics:

- Spark's Machine Learning libraries may not contain specific models that a Data Scientist needs. For those cases, workarounds would include using a sparklyr extension, like [H2O](https://github.com/h2oai/rsparkling) can also be considered or collect a sample of the data into R memory for modeling.

- Spark does not have visualization functionality, currently the best approach is to collect pre-calculated data into R for plotting.  A good way to drastically reduce the number of rows being brought back into memory is to push as much computation as possible to Spark, so that the results can then be plotted.  For example, the bins of a Histogram can be calculated in Spark and then bring back just the final bucket values into R for visualization, here is a sample code: [sparkDemos/Histogram](https://github.com/rstudio/sparkDemos/blob/master/prod/presentations/cloudera/sqlvis_histogram.R)

- A particular use case may require a different way of scaling analytics, we have published an article that provides a very good overview of what options are available: [R for Enterprise: How to Scale Your Analytics Using R](https://www.rstudio.com/rviews/2016/12/21/r-for-enterprise-how-to-scale-your-analytics-using-r/)

## R for Data Science Toolchain with Spark

With sparklyr, the Data Scientist will be able to access the Data Lake's data, and also gain an additional, very powerful **understand** layer with Spark.  Along [RStudio IDE](https://www.rstudio.com/products/rstudio/) and the [tidyverse](http://tidyverse.org/) packages, the Data Scientist will have an excellent toolbox to analyze data big and small.

<center><p><img src="images/slide-5.png" align='center'/></p></center>




