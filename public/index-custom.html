<!DOCTYPE html>
<html>
<head>
  <script>
    theBaseUrl = location.origin + "/";
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
  <title>sparklyr: R interface for Apache Spark</title>
  <meta name="generator" content="Hugo 0.26" />

  
  <meta name="description" content="Documentation for the TensorFlow for R interface">
  
  <link rel="canonical" href="/index-custom.html">
  
  <meta name="author" content="J.J. Allaire">
  

  <meta property="og:url" content="/index-custom.html">
  <meta property="og:title" content="sparklyr">
  <meta name="apple-mobile-web-app-title" content="sparklyr">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <style>
    @font-face {
      font-family: 'Icon';
      src: url('fonts/icon.eot?52m981');
      src: url('fonts/icon.eot?#iefix52m981')
      format('embedded-opentype'),
      url('fonts/icon.woff?52m981')
      format('woff'),
      url('fonts/icon.ttf?52m981')
      format('truetype'),
      url('fonts/icon.svg?52m981#icon')
      format('svg');
      font-weight: normal;
      font-style: normal;
    }
  </style>


  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/site-styles.css">

  <script src="/js/vendor.js"></script>

  <script src="/js/app.js"></script>
  
  <link rel="shortcut icon" href="/images/favicon.png">

  
</head>
<body>


<div class="single-page">
  <nav data-gumshoe-header aria-label="Header" id="header" class="top-menu">
  <div class="left">
     <a href = "/" data-parenturl="" class="top-menu-item site-title">sparklyr</a>
     <span class="logo-from">from</span>
     <a href="/">
      <div id="logo" class="logo"></div>
    </a>
  </div>

  
  

  <div class="right">
    <div class="top-menu-items" id="top-menu-items">
      
        <a href="/" data-parenturl="/" class="top-menu-item">Home</a>
      
        <a href="/keras/" data-parenturl="/keras/" class="top-menu-item">Keras</a>
      
        <a href="/tfestimators/" data-parenturl="/tfestimators/" class="top-menu-item">Estimators</a>
      
        <a href="/tensorflow/" data-parenturl="/tensorflow/" class="top-menu-item">Core</a>
      
        <a href="/examples/" data-parenturl="/examples/" class="top-menu-item">Examples</a>
      
        <a href="/blog.html" data-parenturl="/blog.html" class="top-menu-item">Blog</a>
      
    </div>

    <a href="https://github.com/rstudio/sparklyr" class="github-logo">
      <i class="fa fa-lg fa-github" aria-hidden="true"></i>
    </a>

    <div class="search-button">
      <i class="fa fa-lg fa-search"></i>
    </div>
    
  </div>
</nav>

<nav aria-label="Header" id="mobile-header">
  <a href = "/">sparklyr</a>
  <div class="right">
    <a href="https://github.com/rstudio/sparklyr" class="github-logo">
      <i class="fa fa-lg fa-github" aria-hidden="true"></i>
    </a>
    <div class="hamburger">
      <i class="fa fa-lg fa-bars" aria-hidden="true"></i>
    </div>
  </div>

</nav>

<div id="mobile-menu-container">
<ul id="mobile-menu">
  
    <li>
      <a href="/">Home</a>
       
    </li>
    <ul>
    
    </ul>
  
    <li>
      <a href="/keras/">Keras</a>
       <i class="fa fa-chevron-right" aria-hidden="true"></i>
    </li>
    <ul>
    
      <li>
        <a href="/keras/">Using Keras</a>
        
      </li>
      
    
      <li>
        <a href="/keras/articles/about_keras_models.html">Advanced</a>
        
      </li>
      
    
    </ul>
  
    <li>
      <a href="/tfestimators/">Estimators</a>
       <i class="fa fa-chevron-right" aria-hidden="true"></i>
    </li>
    <ul>
    
      <li>
        <a href="/tfestimators/">Using Estimators</a>
        
      </li>
      
    
      <li>
        <a href="/tfestimators/articles/run_hooks.html">Advanced</a>
        
      </li>
      
    
    </ul>
  
    <li>
      <a href="/tensorflow/">Core</a>
       <i class="fa fa-chevron-right" aria-hidden="true"></i>
    </li>
    <ul>
    
      <li>
        <a href="/tensorflow/">Using TensorFlow</a>
        
      </li>
      
    
      <li>
        <a href="/tensorflow/articles/installation.html">Installation</a>
        
      </li>
      
    
      <li>
        <a href="/tensorflow/articles/tutorial_mnist_beginners.html">Tutorials</a>
        
      </li>
      
    
      <li>
        <a href="/tensorflow/articles/howto_summaries_and_tensorboard.html">TensorBoard</a>
        
      </li>
      
    
    </ul>
  
    <li>
      <a href="/examples/">Examples</a>
       
    </li>
    <ul>
    
    </ul>
  
    <li>
      <a href="/blog.html">Blog</a>
       
    </li>
    <ul>
    
    </ul>
  
</ul>
</div>

<div id="search-bar" class="search-bar">
  <p class="search-bar__icon"><i class="fa fa-lg fa-search"></i> </p>
  <div class="search-bar__input">
      <input type="text" name="search" class="st-default-search-input">
  </div>
  <div class="search-bar__exit">
    <i class="fa fa-lg fa-times"></i>
  </div>

  <div class="inline-search-results">
    <ul>

    </ul>
  </div>
</div>



</div>

<div class="page documentation">
    <div class="side-menu" id="side-menu">
    
    
    
    </div>
    <div class="content">

      <h1 class="content-header">sparklyr: R interface for Apache Spark </h1>

      <div class="markdowned">
        <p><a href="https://travis-ci.org/rstudio/sparklyr"><img src="https://travis-ci.org/rstudio/sparklyr.svg?branch=master" alt="Build Status"></a> <a href="https://cran.r-project.org/package=sparklyr"><img src="https://www.r-pkg.org/badges/version/sparklyr" alt="CRAN_Status_Badge"></a> <a href="https://codecov.io/gh/rstudio/sparklyr"><img src="https://codecov.io/gh/rstudio/sparklyr/branch/master/graph/badge.svg" alt="codecov"></a> <a href="https://gitter.im/rstudio/sparklyr?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge"><img src="https://badges.gitter.im/rstudio/sparklyr.svg" alt="Join the chat at https://gitter.im/rstudio/sparklyr"></a></p>
<p><img src="tools/readme/sparklyr-illustration.png" width="364" height="197" align="right"></p>
<ul>
<li>Connect to <a href="http://spark.apache.org/">Spark</a> from R. The sparklyr package provides a <br> complete <a href="https://github.com/hadley/dplyr">dplyr</a> backend.</li>
<li>Filter and aggregate Spark datasets then bring them into R for <br> analysis and visualization.</li>
<li>Use Spark’s distributed <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> library from R.</li>
<li>Create <a href="http://spark.rstudio.com/extensions.html">extensions</a> that call the full Spark API and provide <br> interfaces to Spark packages.</li>
</ul>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>You can install the <strong>sparklyr</strong> package from CRAN as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="st">"sparklyr"</span>)</code></pre></div>
<p>You should also install a local version of Spark for development purposes:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
<span class="kw">spark_install</span>(<span class="dt">version =</span> <span class="st">"2.1.0"</span>)</code></pre></div>
<p>To upgrade to the latest version of sparklyr, run the following command and restart your r session:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/devtools/topics/install_github">install_github</a></span>(<span class="st">"rstudio/sparklyr"</span>)</code></pre></div>
<p>If you use the RStudio IDE, you should also download the latest <a href="https://www.rstudio.com/products/rstudio/download/preview/">preview release</a> of the IDE which includes several enhancements for interacting with Spark (see the <a href="#rstudio-ide">RStudio IDE</a> section below for more details).</p>
</div>
<div id="connecting-to-spark" class="section level2">
<h2>Connecting to Spark</h2>
<p>You can connect to both local instances of Spark as well as remote Spark clusters. Here we’ll connect to a local instance of Spark via the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html">spark_connect</a> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"local"</span>)</code></pre></div>
<p>The returned Spark connection (<code>sc</code>) provides a remote dplyr data source to the Spark cluster.</p>
<p>For more information on connecting to remote Spark clusters see the <a href="http://spark.rstudio.com/deployment.html">Deployment</a> section of the sparklyr website.</p>
</div>
<div id="using-dplyr" class="section level2">
<h2>Using dplyr</h2>
<p>We can now use all of the available dplyr verbs against the tables within the cluster.</p>
<p>We’ll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">install.packages</span>(<span class="kw">c</span>(<span class="st">"nycflights13"</span>, <span class="st">"Lahman"</span>))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
iris_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, iris)
flights_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, nycflights13<span class="op">::</span>flights, <span class="st">"flights"</span>)
batting_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, Lahman<span class="op">::</span>Batting, <span class="st">"batting"</span>)
<span class="kw">src_tbls</span>(sc)</code></pre></div>
<pre><code>## [1] "batting" "flights" "iris"</code></pre>
<p>To start with here’s a simple filtering example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># filter by departure delay and print the first few records</span>
flights_tbl <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(dep_delay <span class="op">==</span><span class="st"> </span><span class="dv">2</span>)</code></pre></div>
<pre><code>## # Source:   lazy query [?? x 19]
## # Database: spark_connection
##     year month   day dep_time sched_dep_time dep_delay arr_time
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;
##  1  2013     1     1      517            515         2      830
##  2  2013     1     1      542            540         2      923
##  3  2013     1     1      702            700         2     1058
##  4  2013     1     1      715            713         2      911
##  5  2013     1     1      752            750         2     1025
##  6  2013     1     1      917            915         2     1206
##  7  2013     1     1      932            930         2     1219
##  8  2013     1     1     1028           1026         2     1350
##  9  2013     1     1     1042           1040         2     1325
## 10  2013     1     1     1231           1229         2     1523
## # ... with more rows, and 12 more variables: sched_arr_time &lt;int&gt;,
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,
## #   minute &lt;dbl&gt;, time_hour &lt;dbl&gt;</code></pre>
<p><a href="https://CRAN.R-project.org/package=dplyr">Introduction to dplyr</a> provides additional dplyr examples you can try. For example, consider the last example from the tutorial which plots data on flight delays:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">delay &lt;-<span class="st"> </span>flights_tbl <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">group_by</span>(tailnum) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">n</span>(), <span class="dt">dist =</span> <span class="kw">mean</span>(distance), <span class="dt">delay =</span> <span class="kw">mean</span>(arr_delay)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(count <span class="op">&gt;</span><span class="st"> </span><span class="dv">20</span>, dist <span class="op">&lt;</span><span class="st"> </span><span class="dv">2000</span>, <span class="op">!</span><span class="kw">is.na</span>(delay)) <span class="op">%&gt;%</span>
<span class="st">  </span>collect

<span class="co"># plot delays</span>
<span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(delay, <span class="kw">aes</span>(dist, delay)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">size =</span> count), <span class="dt">alpha =</span> <span class="dv">1</span><span class="op">/</span><span class="dv">2</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_smooth</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_size_area</span>(<span class="dt">max_size =</span> <span class="dv">2</span>)</code></pre></div>
<pre><code>## `geom_smooth()` using method = 'gam'</code></pre>
<p><img src="tools/readme/ggplot2-1.png" width="672"></p>
<div id="window-functions" class="section level3">
<h3>Window Functions</h3>
<p>dplyr <a href="https://CRAN.R-project.org/package=dplyr">window functions</a> are also supported, for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">batting_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(playerID, yearID, teamID, G, AB<span class="op">:</span>H) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(playerID, yearID, teamID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(playerID) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(<span class="kw">min_rank</span>(<span class="kw">desc</span>(H)) <span class="op">&lt;=</span><span class="st"> </span><span class="dv">2</span> <span class="op">&amp;</span><span class="st"> </span>H <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)</code></pre></div>
<pre><code>## # Source:     lazy query [?? x 7]
## # Database:   spark_connection
## # Groups:     playerID
## # Ordered by: playerID, yearID, teamID
##     playerID yearID teamID     G    AB     R     H
##        &lt;chr&gt;  &lt;int&gt;  &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
##  1 aaronha01   1959    ML1   154   629   116   223
##  2 aaronha01   1963    ML1   161   631   121   201
##  3 abbotji01   1999    MIL    20    21     0     2
##  4 abnersh01   1992    CHA    97   208    21    58
##  5 abnersh01   1990    SDN    91   184    17    45
##  6 acklefr01   1963    CHA     2     5     0     1
##  7 acklefr01   1964    CHA     3     1     0     1
##  8 adamecr01   2016    COL   121   225    25    49
##  9 adamecr01   2015    COL    26    53     4    13
## 10 adamsac01   1943    NY1    70    32     3     4
## # ... with more rows</code></pre>
<p>For additional documentation on using dplyr with Spark see the <a href="http://spark.rstudio.com/dplyr.html">dplyr</a> section of the sparklyr website.</p>
</div>
</div>
<div id="using-sql" class="section level2">
<h2>Using SQL</h2>
<p>It’s also possible to execute SQL queries directly against tables within a Spark cluster. The <code>spark_connection</code> object implements a <a href="https://github.com/rstats-db/DBI">DBI</a> interface for Spark, so you can use <code>dbGetQuery</code> to execute SQL and return the result as an R data frame:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(DBI)
iris_preview &lt;-<span class="st"> </span><span class="kw">dbGetQuery</span>(sc, <span class="st">"SELECT * FROM iris LIMIT 10"</span>)
iris_preview</code></pre></div>
<pre><code>##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
## 1           5.1         3.5          1.4         0.2  setosa
## 2           4.9         3.0          1.4         0.2  setosa
## 3           4.7         3.2          1.3         0.2  setosa
## 4           4.6         3.1          1.5         0.2  setosa
## 5           5.0         3.6          1.4         0.2  setosa
## 6           5.4         3.9          1.7         0.4  setosa
## 7           4.6         3.4          1.4         0.3  setosa
## 8           5.0         3.4          1.5         0.2  setosa
## 9           4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa</code></pre>
</div>
<div id="machine-learning" class="section level2">
<h2>Machine Learning</h2>
<p>You can orchestrate machine learning algorithms in a Spark cluster via the <a href="http://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> functions within <strong>sparklyr</strong>. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.</p>
<p>Here’s an example where we use <a href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html">ml_linear_regression</a> to fit a linear regression model. We’ll use the built-in <code>mtcars</code> dataset, and see if we can predict a car’s fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>). We’ll assume in each case that the relationship between <code>mpg</code> and each of our features is linear.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># copy mtcars into spark</span>
mtcars_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars)

<span class="co"># transform our data set, and then partition into 'training', 'test'</span>
partitions &lt;-<span class="st"> </span>mtcars_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(hp <span class="op">&gt;=</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cyl8 =</span> cyl <span class="op">==</span><span class="st"> </span><span class="dv">8</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sdf_partition</span>(<span class="dt">training =</span> <span class="fl">0.5</span>, <span class="dt">test =</span> <span class="fl">0.5</span>, <span class="dt">seed =</span> <span class="dv">1099</span>)

<span class="co"># fit a linear model to the training dataset</span>
fit &lt;-<span class="st"> </span>partitions<span class="op">$</span>training <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_linear_regression</span>(<span class="dt">response =</span> <span class="st">"mpg"</span>, <span class="dt">features =</span> <span class="kw">c</span>(<span class="st">"wt"</span>, <span class="st">"cyl"</span>))</code></pre></div>
<pre><code>## * No rows dropped by 'na.omit' call</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit</code></pre></div>
<pre><code>## Call: ml_linear_regression(., response = "mpg", features = c("wt", "cyl"))
## 
## Coefficients:
## (Intercept)          wt         cyl 
##   33.499452   -2.818463   -0.923187</code></pre>
<p>For linear regression models produced by Spark, we can use <code>summary()</code> to learn a bit more about the quality of our fit, and the statistical significance of each of our predictors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## Call: ml_linear_regression(., response = "mpg", features = c("wt", "cyl"))
## 
## Deviance Residuals::
##    Min     1Q Median     3Q    Max 
## -1.752 -1.134 -0.499  1.296  2.282 
## 
## Coefficients:
##             Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 33.49945    3.62256  9.2475 0.0002485 ***
## wt          -2.81846    0.96619 -2.9171 0.0331257 *  
## cyl         -0.92319    0.54639 -1.6896 0.1518998    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-Squared: 0.8274
## Root Mean Squared Error: 1.422</code></pre>
<p>Spark machine learning supports a wide array of algorithms and feature transformations and as illustrated above it’s easy to chain these functions together with dplyr pipelines. To learn more see the <a href="mllib.html">machine learning</a> section.</p>
</div>
<div id="reading-and-writing-data" class="section level2">
<h2>Reading and Writing Data</h2>
<p>You can read and write data in CSV, JSON, and Parquet formats. Data can be stored in HDFS, S3, or on the local filesystem of cluster nodes.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">temp_csv &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".csv"</span>)
temp_parquet &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".parquet"</span>)
temp_json &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".json"</span>)

<span class="kw">spark_write_csv</span>(iris_tbl, temp_csv)
iris_csv_tbl &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">"iris_csv"</span>, temp_csv)

<span class="kw">spark_write_parquet</span>(iris_tbl, temp_parquet)
iris_parquet_tbl &lt;-<span class="st"> </span><span class="kw">spark_read_parquet</span>(sc, <span class="st">"iris_parquet"</span>, temp_parquet)

<span class="kw">spark_write_json</span>(iris_tbl, temp_json)
iris_json_tbl &lt;-<span class="st"> </span><span class="kw">spark_read_json</span>(sc, <span class="st">"iris_json"</span>, temp_json)

<span class="kw">src_tbls</span>(sc)</code></pre></div>
<pre><code>## [1] "batting"      "flights"      "iris"         "iris_csv"    
## [5] "iris_json"    "iris_parquet" "mtcars"</code></pre>
</div>
<div id="distributed-r" class="section level2">
<h2>Distributed R</h2>
<p>You can execute arbitrary r code across your cluster using <code>spark_apply</code>. For example, we can apply <code>rgamma</code> over <code>iris</code> as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_apply</span>(iris_tbl, <span class="cf">function</span>(data) {
  data[<span class="dv">1</span><span class="op">:</span><span class="dv">4</span>] <span class="op">+</span><span class="st"> </span><span class="kw">rgamma</span>(<span class="dv">1</span>,<span class="dv">2</span>)
})</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_1504495178f6&gt; [?? x 4]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
##  1     7.057976    5.457976     3.357976    2.157976
##  2     6.857976    4.957976     3.357976    2.157976
##  3     6.657976    5.157976     3.257976    2.157976
##  4     6.557976    5.057976     3.457976    2.157976
##  5     6.957976    5.557976     3.357976    2.157976
##  6     7.357976    5.857976     3.657976    2.357976
##  7     6.557976    5.357976     3.357976    2.257976
##  8     6.957976    5.357976     3.457976    2.157976
##  9     6.357976    4.857976     3.357976    2.157976
## 10     6.857976    5.057976     3.457976    2.057976
## # ... with more rows</code></pre>
<p>You can also group by columns to perform an operation over each group of rows and make use of any package within the closure:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_apply</span>(
  iris_tbl,
  <span class="cf">function</span>(e) broom<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/broom/topics/tidy">tidy</a></span>(<span class="kw">lm</span>(Petal_Width <span class="op">~</span><span class="st"> </span>Petal_Length, e)),
  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"term"</span>, <span class="st">"estimate"</span>, <span class="st">"std.error"</span>, <span class="st">"statistic"</span>, <span class="st">"p.value"</span>),
  <span class="dt">group_by =</span> <span class="st">"Species"</span>
)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_15045d601c36&gt; [?? x 6]
## # Database: spark_connection
##      Species         term    estimate  std.error  statistic      p.value
##        &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor  (Intercept) -0.08428835 0.16070140 -0.5245029 6.023428e-01
## 2 versicolor Petal_Length  0.33105360 0.03750041  8.8279995 1.271916e-11
## 3  virginica  (Intercept)  1.13603130 0.37936622  2.9945505 4.336312e-03
## 4  virginica Petal_Length  0.16029696 0.06800119  2.3572668 2.253577e-02
## 5     setosa  (Intercept) -0.04822033 0.12164115 -0.3964146 6.935561e-01
## 6     setosa Petal_Length  0.20124509 0.08263253  2.4354220 1.863892e-02</code></pre>
</div>
<div id="extensions" class="section level2">
<h2>Extensions</h2>
<p>The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. Since Spark is a general purpose cluster computing system there are many potential applications for extensions (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).</p>
<p>Here’s a simple example that wraps a Spark text file line counting function with an R function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># write a CSV </span>
tempfile &lt;-<span class="st"> </span><span class="kw">tempfile</span>(<span class="dt">fileext =</span> <span class="st">".csv"</span>)
<span class="kw">write.csv</span>(nycflights13<span class="op">::</span>flights, tempfile, <span class="dt">row.names =</span> <span class="ot">FALSE</span>, <span class="dt">na =</span> <span class="st">""</span>)

<span class="co"># define an R interface to Spark line counting</span>
count_lines &lt;-<span class="st"> </span><span class="cf">function</span>(sc, path) {
  <span class="kw">spark_context</span>(sc) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">invoke</span>(<span class="st">"textFile"</span>, path, 1L) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">invoke</span>(<span class="st">"count"</span>)
}

<span class="co"># call spark to count the lines of the CSV</span>
<span class="kw">count_lines</span>(sc, tempfile)</code></pre></div>
<pre><code>## [1] 336777</code></pre>
<p>To learn more about creating extensions see the <a href="http://spark.rstudio.com/extensions.html">Extensions</a> section of the sparklyr website.</p>
</div>
<div id="table-utilities" class="section level2">
<h2>Table Utilities</h2>
<p>You can cache a table into memory with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tbl_cache</span>(sc, <span class="st">"batting"</span>)</code></pre></div>
<p>and unload from memory using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tbl_uncache</span>(sc, <span class="st">"batting"</span>)</code></pre></div>
</div>
<div id="connection-utilities" class="section level2">
<h2>Connection Utilities</h2>
<p>You can view the Spark web console using the <code>spark_web</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_web</span>(sc)</code></pre></div>
<p>You can show the log using the <code>spark_log</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_log</span>(sc, <span class="dt">n =</span> <span class="dv">10</span>)</code></pre></div>
<pre><code>## 17/09/11 21:47:07 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks
## 17/09/11 21:47:07 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 112, localhost, executor driver, partition 0, PROCESS_LOCAL, 5995 bytes)
## 17/09/11 21:47:07 INFO Executor: Running task 0.0 in stage 72.0 (TID 112)
## 17/09/11 21:47:07 INFO HadoopRDD: Input split: file:/C:/Users/edgar/AppData/Local/Temp/RtmpmoPUcW/file150455851bec.csv:0+33649883
## 17/09/11 21:47:07 INFO BlockManagerInfo: Removed broadcast_80_piece0 on 127.0.0.1:61728 in memory (size: 15.0 KB, free: 385.9 MB)
## 17/09/11 21:47:07 INFO Executor: Finished task 0.0 in stage 72.0 (TID 112). 1019 bytes result sent to driver
## 17/09/11 21:47:07 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 112) in 253 ms on localhost (executor driver) (1/1)
## 17/09/11 21:47:07 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool 
## 17/09/11 21:47:07 INFO DAGScheduler: ResultStage 72 (count at &lt;unknown&gt;:0) finished in 0.253 s
## 17/09/11 21:47:07 INFO DAGScheduler: Job 49 finished: count at &lt;unknown&gt;:0, took 0.269251 s</code></pre>
<p>Finally, we disconnect from Spark:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_disconnect</span>(sc)</code></pre></div>
</div>
<div id="rstudio-ide" class="section level2">
<h2>RStudio IDE</h2>
<p>The latest RStudio <a href="https://www.rstudio.com/products/rstudio/download/preview/">Preview Release</a> of the RStudio IDE includes integrated support for Spark and the sparklyr package, including tools for:</p>
<ul>
<li>Creating and managing Spark connections</li>
<li>Browsing the tables and columns of Spark DataFrames</li>
<li>Previewing the first 1,000 rows of Spark DataFrames</li>
</ul>
<p>Once you’ve installed the sparklyr package, you should find a new <strong>Spark</strong> pane within the IDE. This pane includes a <strong>New Connection</strong> dialog which can be used to make connections to local or remote Spark instances:</p>
<p><img src="tools/readme/spark-connect.png" class="screenshot" width="389"></p>
<p>Once you’ve connected to Spark you’ll be able to browse the tables contained within the Spark cluster and preview Spark DataFrames using the standard RStudio data viewer:</p>
<p><img src="tools/readme/spark-dataview.png" class="screenshot" width="639"></p>
<p>You can also connect to Spark through <a href="http://livy.io">Livy</a> through a new connection dialog:</p>
<p><img src="tools/readme/spark-connect-livy.png" class="screenshot" width="389"></p>
<div style="margin-bottom: 15px;">

</div>
<p>The RStudio IDE features for sparklyr are available now as part of the <a href="https://www.rstudio.com/products/rstudio/download/preview/">RStudio Preview Release</a>.</p>
</div>
<div id="using-h2o" class="section level2">
<h2>Using H2O</h2>
<p><a href="https://cran.r-project.org/package=rsparkling">rsparkling</a> is a CRAN package from <a href="http://h2o.ai">H2O</a> that extends <a href="http://spark.rstudio.com">sparklyr</a> to provide an interface into <a href="https://github.com/h2oai/sparkling-water">Sparkling Water</a>. For instance, the following example installs, configures and runs <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html">h2o.glm</a>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">rsparkling.sparklingwater.version =</span> <span class="st">"2.1.0"</span>)

<span class="kw">library</span>(rsparkling)
<span class="kw">library</span>(sparklyr)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(h2o)

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"local"</span>, <span class="dt">version =</span> <span class="st">"2.1.0"</span>)
mtcars_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars, <span class="st">"mtcars"</span>)

mtcars_h2o &lt;-<span class="st"> </span><span class="kw">as_h2o_frame</span>(sc, mtcars_tbl, <span class="dt">strict_version_check =</span> <span class="ot">FALSE</span>)

mtcars_glm &lt;-<span class="st"> </span><span class="kw">h2o.glm</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="st">"wt"</span>, <span class="st">"cyl"</span>), 
                      <span class="dt">y =</span> <span class="st">"mpg"</span>,
                      <span class="dt">training_frame =</span> mtcars_h2o,
                      <span class="dt">lambda_search =</span> <span class="ot">TRUE</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtcars_glm</code></pre></div>
<pre><code>## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model ID:  GLM_model_R_1505184449758_1 
## GLM Model: summary
##     family     link                              regularization
## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )
##                                                                lambda_search
## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0
##   number_of_predictors_total number_of_active_predictors
## 1                          2                           2
##   number_of_iterations                                training_frame
## 1                    0 frame_rdd_29_bfb52c89d4a547da81a54a109cc54121
## 
## Coefficients: glm coefficients
##       names coefficients standardized_coefficients
## 1 Intercept    38.941654                 20.090625
## 2       cyl    -1.468783                 -2.623132
## 3        wt    -3.034558                 -2.969186
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. **
## 
## MSE:  6.017684
## RMSE:  2.453097
## MAE:  1.940985
## RMSLE:  0.1114801
## Mean Residual Deviance :  6.017684
## R^2 :  0.8289895
## Null Deviance :1126.047
## Null D.o.F. :31
## Residual Deviance :192.5659
## Residual D.o.F. :29
## AIC :156.2425</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_disconnect</span>(sc)</code></pre></div>
</div>
<div id="connecting-through-livy" class="section level2">
<h2>Connecting through Livy</h2>
<p><a href="https://github.com/cloudera/livy">Livy</a> enables remote connections to Apache Spark clusters. Connecting to Spark clusters through Livy is <strong>under experimental development</strong> in <code>sparklyr</code>. Please post any feedback or questions as a GitHub issue as needed.</p>
<p>Before connecting to Livy, you will need the connection information to an existing service running Livy. Otherwise, to test <code>livy</code> in your local environment, you can install it and run it locally as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">livy_install</span>()</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">livy_service_start</span>()</code></pre></div>
<p>To connect, use the Livy service address as <code>master</code> and <code>method = "livy"</code> in <code>spark_connect</code>. Once connection completes, use <code>sparklyr</code> as usual, for instance:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"http://localhost:8998"</span>, <span class="dt">method =</span> <span class="st">"livy"</span>)
<span class="kw">copy_to</span>(sc, iris)

<span class="kw">spark_disconnect</span>(sc)</code></pre></div>
<p>Once you are done using <code>livy</code> locally, you should stop this service with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">livy_service_stop</span>()</code></pre></div>
<p>To connect to remote <code>livy</code> clusters that support basic authentication connect as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">livy_config_auth</span>(<span class="st">"&lt;username&gt;"</span>, <span class="st">"&lt;password"</span><span class="op">&gt;</span>)
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"&lt;address&gt;"</span>, <span class="dt">method =</span> <span class="st">"livy"</span>, <span class="dt">config =</span> config)
<span class="kw">spark_disconnect</span>(sc)</code></pre></div>
</div>

      </div>
      
      
<footer>



</footer>
      
    </div>
</div>

</body>
</html>

