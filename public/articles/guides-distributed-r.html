<!DOCTYPE html>
<html>
<head>
  <script>
    theBaseUrl = location.origin + "/";
  </script>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,user-scalable=no,initial-scale=1,maximum-scale=1">
  <title>Distributing R Computations</title>
  <meta name="generator" content="Hugo 0.26" />

  
  <meta name="description" content="Documentation for the Spark for R interface">
  
  <link rel="canonical" href="/articles/guides-distributed-r.html">
  
  <meta name="author" content="Javier Luraschii">
  

  <meta property="og:url" content="/articles/guides-distributed-r.html">
  <meta property="og:title" content="sparklyr">
  <meta name="apple-mobile-web-app-title" content="sparklyr">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <style>
    @font-face {
      font-family: 'Icon';
      src: url('fonts/icon.eot?52m981');
      src: url('fonts/icon.eot?#iefix52m981')
      format('embedded-opentype'),
      url('fonts/icon.woff?52m981')
      format('woff'),
      url('fonts/icon.ttf?52m981')
      format('truetype'),
      url('fonts/icon.svg?52m981#icon')
      format('svg');
      font-weight: normal;
      font-style: normal;
    }
  </style>


  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="/css/site-styles.css">

  <script src="/js/vendor.js"></script>

  <script src="/js/app.js"></script>
  
  <link rel="shortcut icon" href="/images/favicon.png">

  
</head>
<body>


<div class="single-page">
  <nav data-gumshoe-header aria-label="Header" id="header" class="top-menu">
  <div class="left">
     <a href = "/" data-parenturl="" class="top-menu-item site-title">sparklyr</a>
     <span class="logo-from">from</span>
     <a href="/">
      <div id="logo" class="logo"></div>
    </a>
  </div>

  
  

  <div class="right">
    <div class="top-menu-items" id="top-menu-items">
      
        <a href="/articles/guides-dplyr" data-parenturl="/articles/guides-dplyr" class="top-menu-item">dplyr</a>
      
    </div>

    <a href="https://github.com/rstudio/sparklyr" class="github-logo">
      <i class="fa fa-lg fa-github" aria-hidden="true"></i>
    </a>

    <div class="search-button">
      <i class="fa fa-lg fa-search"></i>
    </div>
    
  </div>
</nav>

<nav aria-label="Header" id="mobile-header">
  <a href = "/">sparklyr</a>
  <div class="right">
    <a href="https://github.com/rstudio/sparklyr" class="github-logo">
      <i class="fa fa-lg fa-github" aria-hidden="true"></i>
    </a>
    <div class="hamburger">
      <i class="fa fa-lg fa-bars" aria-hidden="true"></i>
    </div>
  </div>

</nav>

<div id="mobile-menu-container">
<ul id="mobile-menu">
  
    <li>
      <a href="/articles/guides-dplyr">dplyr</a>
       
    </li>
    <ul>
    
    </ul>
  
</ul>
</div>

<div id="search-bar" class="search-bar">
  <p class="search-bar__icon"><i class="fa fa-lg fa-search"></i> </p>
  <div class="search-bar__input">
      <input type="text" name="search" class="st-default-search-input">
  </div>
  <div class="search-bar__exit">
    <i class="fa fa-lg fa-times"></i>
  </div>

  <div class="inline-search-results">
    <ul>

    </ul>
  </div>
</div>



</div>

<div class="page documentation">
    <div class="side-menu" id="side-menu">
    
    
    
    </div>
    <div class="content">

      <h1 class="content-header">Distributing R Computations </h1>

      <div class="markdowned">
        <div id="overview" class="section level2">
<h2>Overview</h2>
<p><strong>sparklyr</strong> provides support to run arbitrary R code at scale within your Spark Cluster through <code>spark_apply()</code>. This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor <a href="https://spark-packages.org/">Spark Packages</a>.</p>
<p><code>spark_apply()</code> applies an R function to a Spark object (typically, a Spark DataFrame). Spark objects are partitioned so they can be distributed across a cluster. You can use <code>spark_apply</code> with the default partitions or you can define your own partitions with the <code>group_by</code> argument. Your R function must return another Spark DataFrame. <code>spark_apply</code> will run your R function on each partition and output a single Spark DataFrame.</p>
<div id="apply-an-r-function-to-a-spark-object" class="section level3">
<h3>Apply an R function to a Spark Object</h3>
<p>Lets run a simple example. We will apply the identify function, <code>I()</code>, over a list of numbers we created with the <code>sdf_len</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"local"</span>)

<span class="kw">sdf_len</span>(sc, <span class="dv">5</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) <span class="kw">I</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d50ff65b3d&gt; [?? x 1]
## # Database: spark_connection
##      id
##   &lt;dbl&gt;
## 1     1
## 2     2
## 3     3
## 4     4
## 5     5</code></pre>
<p>Your R function should be designed to operate on an R <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html">data frame</a>. The R function passed to <code>spark_apply</code> expects a DataFrame and will return an object that can be cast as a DataFrame. We can use the <code>class</code> function to verify the class of the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) <span class="kw">class</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d5047027354&gt; [?? x 1]
## # Database: spark_connection
##           id
##        &lt;chr&gt;
## 1 data.frame</code></pre>
<p>Spark will partition your data by hash or range so it can be distributed across a cluster. In the following example we create two partitions and count the number of rows in each partition. Then we print the first record in each partition.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, trees, <span class="dt">repartition =</span> <span class="dv">2</span>)

trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) <span class="kw">nrow</span>(e), <span class="dt">names =</span> <span class="st">"n"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d507b1f3335&gt; [?? x 1]
## # Database: spark_connection
##       n
##   &lt;int&gt;
## 1    16
## 2    15</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) <span class="kw">head</span>(e, <span class="dv">1</span>))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d501d096eee&gt; [?? x 3]
## # Database: spark_connection
##   Girth Height Volume
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1   8.3     70   10.3
## 2   8.6     65   10.3</code></pre>
<p>We can apply any arbitrary function to the partitions in the Spark DataFrame. For instance, we can scale or jitter the columns. Notice that <code>spark_apply</code> applies the R function to all partitions and returns a single DataFrame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) <span class="kw">scale</span>(e))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d5044714de3&gt; [?? x 3]
## # Database: spark_connection
##         Girth      Height     Volume
##         &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1 -1.4482330 -0.99510521 -1.1503645
##  2 -1.3021313 -2.06675697 -1.1558670
##  3 -0.7469449  0.68891899 -0.6826528
##  4 -0.6592839 -1.60747764 -0.8587325
##  5 -0.6300635  0.53582588 -0.4735581
##  6 -0.5716229  0.38273277 -0.3855183
##  7 -0.5424025 -0.07654655 -0.5395880
##  8 -0.3670805 -0.22963966 -0.6661453
##  9 -0.1040975  1.30129143  0.1427209
## 10  0.1296653 -0.84201210 -0.3029809
## # ... with more rows</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) <span class="kw">lapply</span>(e, jitter))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d5074831cc4&gt; [?? x 3]
## # Database: spark_connection
##        Girth   Height   Volume
##        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
##  1  8.282741 69.94553 10.29201
##  2  8.803667 62.82448 10.18630
##  3 10.704693 81.07561 18.81394
##  4 10.994508 65.90851 15.58180
##  5 11.089487 80.05814 22.59422
##  6 11.291752 79.05910 24.20069
##  7 11.411218 76.07703 21.39366
##  8 12.008975 74.95662 19.08762
##  9 12.904847 85.02100 33.79268
## 10 13.713613 71.17844 25.71222
## # ... with more rows</code></pre>
<p>By default <code>spark_apply()</code> derives the column names from the input Spark data frame. Use the <code>names</code> argument to rename or add new columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">trees_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(
    <span class="cf">function</span>(e) <span class="kw">data.frame</span>(<span class="fl">2.54</span> <span class="op">*</span><span class="st"> </span>e<span class="op">$</span>Girth, e),
    <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"Girth(cm)"</span>, <span class="kw">colnames</span>(trees)))</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d50c6c6b9c&gt; [?? x 4]
## # Database: spark_connection
##    `Girth(cm)` Girth Height Volume
##          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1      21.082   8.3     70   10.3
##  2      22.352   8.8     63   10.2
##  3      27.178  10.7     81   18.8
##  4      27.940  11.0     66   15.6
##  5      28.194  11.1     80   22.6
##  6      28.702  11.3     79   24.2
##  7      28.956  11.4     76   21.4
##  8      30.480  12.0     75   19.1
##  9      32.766  12.9     85   33.8
## 10      34.798  13.7     71   25.7
## # ... with more rows</code></pre>
</div>
<div id="group-by" class="section level3">
<h3>Group By</h3>
<p>In some cases you may want to apply your R function to specific groups in your data. For example, suppose you want to compute regression models against specific subgroups. To solve this, you can specify a <code>group_by</code> argument. This example counts the number of rows in <code>iris</code> by species and then fits a simple linear model for each species.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris)

iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(nrow, <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d501f6d418&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;int&gt;
## 1 versicolor           50
## 2  virginica           50
## 3     setosa           50</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(
    <span class="cf">function</span>(e) <span class="kw">summary</span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e))<span class="op">$</span>r.squared,
    <span class="dt">names =</span> <span class="st">"r.squared"</span>,
    <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d5010176622&gt; [?? x 2]
## # Database: spark_connection
##      Species r.squared
##        &lt;chr&gt;     &lt;dbl&gt;
## 1 versicolor 0.6188467
## 2  virginica 0.1037537
## 3     setosa 0.1099785</code></pre>
</div>
</div>
<div id="distributing-packages" class="section level2">
<h2>Distributing Packages</h2>
<p>With <code>spark_apply()</code> you can use any R package inside Spark. For instance, you can use the <a href="https://cran.r-project.org/package=broom">broom</a> package to create a tidy data frame from linear regression output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_apply</span>(
  iris_tbl,
  <span class="cf">function</span>(e) broom<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/broom/topics/tidy">tidy</a></span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e)),
  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">"term"</span>, <span class="st">"estimate"</span>, <span class="st">"std.error"</span>, <span class="st">"statistic"</span>, <span class="st">"p.value"</span>),
  <span class="dt">group_by =</span> <span class="st">"Species"</span>)</code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_2d5032b07438&gt; [?? x 6]
## # Database: spark_connection
##      Species        term  estimate std.error statistic      p.value
##        &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor (Intercept) 1.7812754 0.2838234  6.276000 9.484134e-08
## 2 versicolor Petal_Width 1.8693247 0.2117495  8.827999 1.271916e-11
## 3  virginica (Intercept) 4.2406526 0.5612870  7.555230 1.041600e-09
## 4  virginica Petal_Width 0.6472593 0.2745804  2.357267 2.253577e-02
## 5     setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27
## 6     setosa Petal_Width 0.5464903 0.2243924  2.435422 1.863892e-02</code></pre>
<p>To use R packages inside Spark, your packages must be installed on the worker nodes. The first time you call <code>spark_apply</code> all of the contents in your local <code>.libPaths()</code> will be copied into each Spark worker node via the <code>SparkConf.addFile()</code> function. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting <code>packages = FALSE</code>. Note: packages are not copied in local mode (<code>master="local"</code>) because the packages already exist on the system.</p>
</div>
<div id="handling-errors" class="section level2">
<h2>Handling Errors</h2>
<p>It can be more difficult to troubleshoot R issues in a cluster than in local mode. For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_apply</span>(iris_tbl, <span class="cf">function</span>(e) <span class="kw">stop</span>(<span class="st">"Make this fail"</span>))</code></pre></div>
<pre><code> Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details</code></pre>
<p>In local mode, <code>sparklyr</code> will retrieve the logs for you. The logs point out the real failure as <code>ERROR sparklyr: RScript (4190) Make this fail</code> as you might expect.</p>
<pre><code>---- Output Log ----
(17/07/27 21:24:18 ERROR sparklyr: Worker (2427) is shutting down with exception ,java.net.SocketException: Socket closed)
17/07/27 21:24:18 WARN TaskSetManager: Lost task 0.0 in stage 389.0 (TID 429, localhost, executor driver): 17/07/27 21:27:21 INFO sparklyr: RScript (4190) retrieved 150 rows 
17/07/27 21:27:21 INFO sparklyr: RScript (4190) computing closure 
17/07/27 21:27:21 ERROR sparklyr: RScript (4190) Make this fail </code></pre>
<p>It is worth mentioning that different cluster providers and platforms expose worker logs in different ways. Specific documentation for your environment will point out how to retrieve these logs.</p>
</div>
<div id="requirements" class="section level2">
<h2>Requirements</h2>
<p>The <strong>R Runtime</strong> is expected to be pre-installed in the cluster for <code>spark_apply</code> to function. Failure to install the cluster will trigger a <code>Cannot run program, no such file or directory</code> error while attempting to use <code>spark_apply()</code>. Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.</p>
<p>A <strong>Homogenius Cluster</strong> is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc.</p>
</div>
<div id="configuration" class="section level2">
<h2>Configuration</h2>
<p>The following table describes relevant parameters while making use of <code>spark_apply</code>.</p>
<table>
<colgroup>
<col width="38%">
<col width="61%">
</colgroup>
<thead><tr class="header">
<th>Value</th>
<th>Description</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><code>spark.r.command</code></td>
<td>The path to the R binary. Useful to select from multiple R versions.</td>
</tr>
<tr class="even">
<td><code>sparklyr.worker.gateway.address</code></td>
<td>The gateway address to use under each worker node. Defaults to <code>sparklyr.gateway.address</code>.</td>
</tr>
<tr class="odd">
<td><code>sparklyr.worker.gateway.port</code></td>
<td>The gateway port to use under each worker node. Defaults to <code>sparklyr.gateway.port</code>.</td>
</tr>
</tbody>
</table>
<p>For example, one could make use of an specific R version by running:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
config[[<span class="st">"spark.r.command"</span>]] &lt;-<span class="st"> "&lt;path-to-r-version&gt;"</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"local"</span>, <span class="dt">config =</span> config)
<span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) e)</code></pre></div>
</div>
<div id="limitations" class="section level2">
<h2>Limitations</h2>
<div id="closures" class="section level3">
<h3>Closures</h3>
<p>Closures are serialized using <code>serialize</code>, which is described as “A simple low-level interface for serializing to connections.”. One of the current limitations of <code>serialize</code> is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references <code>external_value</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">external_value &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="kw">spark_apply</span>(iris_tbl, <span class="cf">function</span>(e) e <span class="op">+</span><span class="st"> </span>external_value)</code></pre></div>
</div>
<div id="livy" class="section level3">
<h3>Livy</h3>
<p>Currently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, not operating systems that the cluster machines.</p>
</div>
<div id="computing-over-groups" class="section level3">
<h3>Computing over Groups</h3>
<p>While performing computations over groups, <code>spark_apply()</code> will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use <code><a href="http://dplyr.tidyverse.org/reference/do.html">dplyr::do</a></code> which is currently optimized for large partitions.</p>
</div>
<div id="package-installation" class="section level3">
<h3>Package Installation</h3>
<p>Since packages are copied only once for the duration of the <code>spark_connect()</code> connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, <code>spark_disconnect()</code> the connection, modify packages and reconnect.</p>
</div>
</div>

      </div>
      
      
<footer>



</footer>
      
    </div>
</div>

</body>
</html>

