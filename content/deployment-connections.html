---
title: "Configuring Spark Connections"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
---



<div id="customizing-connections" class="section level2">
<h2>Customizing connections</h2>
<p>A connection to Spark can be customized by setting the values of certain Spark properties. In <code>sparklyr</code>, Spark properties can be set by using the <code>config</code> argument in the <code>spark_connect()</code> function.</p>
<p>By default, <code>spark_connect()</code> uses <code>spark_config()</code> as the default configuration. But that can be customized as shown in the example code below. Because of the unending number of possible combinations, <code>spark_config()</code> contains only a basic configuration, so it will be very likely that additional settings will be needed to properly connect to the cluster.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()   <span class="co"># Load variable with spark_config()</span>

conf<span class="op">$</span>spark.executor.memory &lt;-<span class="st"> "16G"</span> <span class="co"># Use `$` to add or set values</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"yarn-client"</span>, 
                    <span class="dt">config =</span> conf)  <span class="co"># Pass the conf variable </span></code></pre></div>
<div id="spark-definitions" class="section level3">
<h3>Spark definitions</h3>
<p>It may be useful to provide some simple definitions for the Spark nomenclature:</p>
<ul>
<li><p><strong>Node:</strong> A server</p></li>
<li><p><strong>Worker Node:</strong> A server that is part of the cluster and are available to run Spark jobs</p></li>
<li><p><strong>Master Node:</strong> The server that coordinates the Worker nodes.</p></li>
<li><p><strong>Executor:</strong> A sort of virtual machine inside a node. <strong>One Node can have multiple Executors</strong>.</p></li>
<li><p><strong>Driver Node:</strong> The Node that initiates the Spark session. Typically, this will be the server where <code>sparklyr</code> is located.</p></li>
<li><p><strong>Driver (Executor):</strong> The Driver Node will also show up in the Executor list.</p></li>
</ul>
</div>
<div id="useful-concepts" class="section level3">
<h3>Useful concepts</h3>
<ul>
<li><p><strong>Spark configuration properties passed by R are just requests</strong> - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session.</p></li>
<li><p><strong>The cluster overrides ‘silently’</strong> - Many times, no errors are returned when more resources than allowed are requested, or if an attempt is made to change a setting fixed by the cluster.</p></li>
</ul>
</div>
</div>
<div id="yarn-cluster" class="section level2">
<h2>YARN cluster</h2>
<div id="background" class="section level3">
<h3>Background</h3>
<p>Using Spark and R inside a Hadoop based <em>Data Lake</em> is becoming a common practice at companies. Currently, <em>there is no good way to manage user connections to the Spark service centrally</em>. There are some caps and settings that can be applied, but in most cases there are configurations that the R user will need to customize.</p>
<p>The <a href="https://spark.apache.org/docs/latest/running-on-yarn.html">Running on YARN</a> page in Spark’s official website is the best place to start for configuration settings reference, please bookmark it. Cluster administrators and users can benefit from this document. If Spark is new to the company, the <a href="https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/">YARN tunning</a> article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works.</p>
</div>
<div id="recommended-properties" class="section level3">
<h3>Recommended properties</h3>
<p>The following are the recommended Spark properties to set when connecting via R:</p>
<ul>
<li><p><strong>spark.executor.memory</strong> - The maximum possible is managed by the YARN cluster. See the <a href="#yarn-executor">Executor Memory Error</a></p></li>
<li><p><strong>spark.executor.cores</strong> - Number of cores assigned per Executor.</p></li>
<li><p><strong>spark.executor.instances</strong> - Number of executors to start. This property is acknowledged by the cluster if <em>spark.dynamicAllocation.enabled</em> is set to “false”.</p></li>
<li><p><strong>spark.dynamicAllocation.enabled</strong> - Overrides the mechanism that Spark provides to dynamically adjust resources. Disabling it provides more control over the number of the Executors that can be started, which in turn impact the amount of storage available for the session. For more information, please see the <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">Dynamic Resource Allocation</a> page in the official Spark website.</p></li>
</ul>
</div>
<div id="connection-example" class="section level3">
<h3>Connection example</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()

conf<span class="op">$</span>spark.executor.memory &lt;-<span class="st"> "300M"</span>
conf<span class="op">$</span>spark.executor.cores &lt;-<span class="st"> </span><span class="dv">2</span>
conf<span class="op">$</span>spark.executor.instances &lt;-<span class="st"> </span><span class="dv">3</span>
conf<span class="op">$</span>spark.dynamicAllocation.enabled &lt;-<span class="st"> "false"</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"yarn-client"</span>, 
                    <span class="dt">spark_home =</span> <span class="st">"/usr/lib/spark/"</span>,
                    <span class="dt">version =</span> <span class="st">"1.6.0"</span>,
                    <span class="dt">config =</span> conf)</code></pre></div>
<div id="executors-page" class="section level4">
<h4>Executors page</h4>
<p>To see how the requested configuration affected the Spark connection, go to the <strong>Executors</strong> page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040.</p>
<p>Notice that 155.3MB per executor are assigned instead of the 300MB requested. This is because the <em>spark.memory.fraction</em> has been fixed by the cluster, plus, there is fixed amount of memory designated for overhead.</p>
<div class="figure">
<img src="images/deployment/connections/yarnclient.png">
</div>
</div>
</div>
<div id="yarn-executor" class="section level3">
<h3>Executor memory error</h3>
<p>Requesting more memory or CPUs for Executors than allowed will return an error. This is one of the exceptions to the cluster’s ‘silent’ overrides. It will return a message similar to this:</p>
<pre><code>    Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'</code></pre>
<p><strong>A cluster’s administrator</strong> is the only person who can make changes to the settings mentioned in the error. If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster’s web UI. Otherwise, changes to those settings are done directly in the <em>yarn-default.xml</em> file.</p>
</div>
<div id="kerberos" class="section level3">
<h3>Kerberos</h3>
<p>There are two options to access a “kerberized” data lake:</p>
<ul>
<li>
<p>Use <em>kinit</em> to get and cache the ticket. After <em>kinit</em> is installed and configured. After <em>kinit</em> is setup, it can used in R via a <code>system()</code> call prior to connecting to the cluster:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system</span>(<span class="st">"echo '&lt;password&gt;' | kinit &lt;username&gt;"</span>)</code></pre></div>
<p>For more information visit this site: <a href="http://directory.apache.org/apacheds/kerberos-ug/4.1-authenticate-kinit.html">Apache - Authenticate with kinit</a></p>
</li>
<li><p>A preferred option may be to use the out-of-the-box integration with Kerberos that the commercial version of <a href="https://www.rstudio.com/products/rstudio-server-pro/">RStudio Server</a> offers.</p></li>
</ul>
</div>
</div>
<div id="standalone-mode" class="section level2">
<h2>Standalone mode</h2>
<div id="recommended-properties-1" class="section level3">
<h3>Recommended properties</h3>
<p>The following are the recommended Spark properties to set when connecting via R:</p>
<p>The default behavior in Standalone mode is to create one executor per worker. So in a 3 worker node cluster, there will be 3 executors setup. The basic properties that can be set are:</p>
<ul>
<li><p><strong>spark.executor.memory</strong> - The requested memory cannot exceed the actual RAM available.</p></li>
<li><p><strong>spark.memory.fraction</strong> - The default is set to 60% of the requested memory per executor. For more information, please see this <a href="https://spark.apache.org/docs/latest/tuning.html#memory-management-overview">Memory Management Overview</a> page in the official Spark website.</p></li>
<li><p><strong>spark.executor.cores</strong> - The requested cores cannot be higher than the cores available in each worker.</p></li>
</ul>
<div id="dynamic-allocation" class="section level4">
<h4>Dynamic Allocation</h4>
<p>If dynamic allocation is disabled, then Spark will attempt to assign all of the available cores evenly across the cluster. The property used is <strong>spark.dynamicAllocation.enabled</strong>.</p>
<p>For example, the Standalone cluster used for this article has 3 worker nodes. Each node has 14.7GB in RAM and 4 cores. This means that there are a total of 12 cores (3 workers with 4 cores) and 44.1GB in RAM (3 workers with 14.7GB in RAM each).</p>
<p>If the <code>spark.executor.cores</code> property is set to 2, and dynamic allocation is disabled, then Spark will spawn 6 executors. The <code>spark.executor.memory</code> property should be set to a level that when the value is multiplied by 6 (number of executors) it will not be over total available RAM. In this case, the value can be safely set to 7GB so that the total memory requested will be 42GB, which is under the available 44.1GB.</p>
</div>
</div>
<div id="connection-example-1" class="section level3">
<h3>Connection example</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>spark.executor.memory &lt;-<span class="st"> "7GB"</span>
conf<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.9</span>
conf<span class="op">$</span>spark.executor.cores &lt;-<span class="st"> </span><span class="dv">2</span>
conf<span class="op">$</span>spark.dynamicAllocation.enabled &lt;-<span class="st"> "false"</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master=</span><span class="st">"spark://master-url:7077"</span>, 
              <span class="dt">version =</span> <span class="st">"2.1.0"</span>,
              <span class="dt">config =</span> conf,
              <span class="dt">spark_home =</span> <span class="st">"/home/ubuntu/spark-2.1.0-bin-hadoop2.7/"</span>)</code></pre></div>
<div id="executors-page-1" class="section level4">
<h4>Executors page</h4>
<p>To see how the requested configuration affected the Spark connection, go to the <strong>Executors</strong> page in the Spark Web UI. Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040:</p>
<div class="figure">
<img src="images/deployment/connections/standalone.png">
</div>
</div>
</div>
</div>
<div id="local-mode" class="section level2">
<h2>Local mode</h2>
<div id="recommended-properties-2" class="section level3">
<h3>Recommended properties</h3>
<p>The following are the recommended Spark properties to set when connecting via R:</p>
<ul>
<li><p><strong>sparklyr.cores.local</strong> - It defaults to using all of the available cores. Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.</p></li>
<li><p><strong>sparklyr.shell.driver-memory</strong> - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.</p></li>
<li><p><strong>spark.memory.fraction</strong> - The default is set to 60% of the requested memory per executor. For more information, please see this <a href="https://spark.apache.org/docs/latest/tuning.html#memory-management-overview">Memory Management Overview</a> page in the official Spark website.</p></li>
</ul>
</div>
<div id="connection-example-2" class="section level3">
<h3>Connection example</h3>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">conf<span class="op">$</span><span class="st">`</span><span class="dt">sparklyr.cores.local</span><span class="st">`</span> &lt;-<span class="st"> </span><span class="dv">4</span>
conf<span class="op">$</span><span class="st">`</span><span class="dt">sparklyr.shell.driver-memory</span><span class="st">`</span> &lt;-<span class="st"> "16G"</span>
conf<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.9</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">"local"</span>, 
                    <span class="dt">version =</span> <span class="st">"2.1.0"</span>,
                    <span class="dt">config =</span> conf)</code></pre></div>
<div id="executors-page-2" class="section level4">
<h4>Executors page</h4>
<p>To see how the requested configuration affected the Spark connection, go to the <strong>Executors</strong> page in the Spark Web UI available in <a href="http://localhost:4040/storage/" class="uri">http://localhost:4040/storage/</a></p>
<div class="figure">
<img src="images/deployment/connections/local.png">
</div>
</div>
</div>
</div>
