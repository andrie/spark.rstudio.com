---
title: "Apply an R Function in Spark"
aliases:
  - reference/sparklyr/latest/spark_apply
---

    <div class="doc-page">

    <div class="doc-page-index">
    <ul data-gumshoe>
    <li><a href="#arguments">Arguments</a></li>
    
    <li><a href="#configuration">Configuration</a></li>
    
    <li><a href="#examples">Examples</a></li>
    </ul>
    </div>

    <div class="doc-page-body">

    
    <p>Applies an R function to a Spark object (typically, a Spark DataFrame).</p>
    

    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='fu'>spark_apply</span>(<span class='no'>x</span>, <span class='no'>f</span>, <span class='kw'>columns</span> <span class='kw'>=</span> <span class='fu'>colnames</span>(<span class='no'>x</span>), <span class='kw'>memory</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>group_by</span> <span class='kw'>=</span> <span class='kw'>NULL</span>,
  <span class='kw'>packages</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>context</span> <span class='kw'>=</span> <span class='kw'>NULL</span>, <span class='no'>...</span>)</code></pre></div>
    
    <h2 id="arguments">Arguments</h2>
    <table class="ref-arguments">

    <colgroup>
      <col class="name" />
      <col class="desc" />
    </colgroup>

    <tr>
      <td>x</td>
      <td><p>An object (usually a <code>spark_tbl</code>) coercable to a Spark DataFrame.</p></td>
    </tr>
    <tr>
      <td>f</td>
      <td><p>A function that transforms a data frame partition into a data frame.
The function <code>f</code> has signature <code>f(df, context, group1, group2, ...)</code> where
<code>df</code> is a data frame with the data to be processed, <code>context</code>
is an optional object passed as the <code>context</code> parameter and <code>group1</code> to
<code>groupN</code> contain the values of the <code>group_by</code> values. When
<code>group_by</code> is not specified, <code>f</code> takes only one argument.</p></td>
    </tr>
    <tr>
      <td>columns</td>
      <td><p>A vector of column names or a named vector of column types for
the transformed object. Defaults to the names from the original object and
adds indexed column names when not enough columns are specified.</p></td>
    </tr>
    <tr>
      <td>memory</td>
      <td><p>Boolean; should the table be cached into memory?</p></td>
    </tr>
    <tr>
      <td>group_by</td>
      <td><p>Column name used to group by data frame partitions.</p></td>
    </tr>
    <tr>
      <td>packages</td>
      <td><p>Boolean to distribute <code>.libPaths()</code> packages to each node,
  a list of packages to distribute, or a package bundle created with
  <code><a href='spark_apply_bundle'>spark_apply_bundle()</a></code>.</p>
<p>For clusters using Livy or Yarn cluster mode, <code>packages</code> must
  point to a package bundle created using <code><a href='spark_apply_bundle'>spark_apply_bundle()</a></code>
  and made available as a Spark file using <code>config$sparklyr.shell.files</code>.</p>
<p>For offline clusters where <code>available.packages()</code> is not available,
  manually download the packages database from
 https://cran.r-project.org/web/packages/packages.rds and set
  <code>Sys.setenv(sparklyr.apply.packagesdb = "&lt;pathl-to-rds&gt;")</code>. Otherwise,
  all packages will be used by default.</p></td>
    </tr>
    <tr>
      <td>context</td>
      <td><p>Optional object to be serialized and passed back to <code>f()</code>.</p></td>
    </tr>
    <tr>
      <td>...</td>
      <td><p>Optional arguments; currently unused.</p></td>
    </tr>
    </table>
    
    <h2 id="configuration">Configuration</h2>

    
    <p><code><a href='spark_config'>spark_config()</a></code> settings can be specified to change the workers
environment.</p>
<p>For instance, to set additional environment variables to each
worker node use the <code>sparklyr.apply.env.*</code> config, to launch workers
without <code>--vanilla</code> use <code>sparklyr.apply.options.vanilla</code> set to
<code>FALSE</code>, to run a custom script before launching Rscript use
<code>sparklyr.apply.options.rscript.before</code>.</p>
    

    <h2 id="examples">Examples</h2>
    <div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class='co'># NOT RUN {</span>
<span class='fu'>library</span>(<span class='no'>sparklyr</span>)
<span class='no'>sc</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='spark-connections'>spark_connect</a></span>(<span class='kw'>master</span> <span class='kw'>=</span> <span class='st'>"local"</span>)

<span class='co'># creates an Spark data frame with 10 elements then multiply times 10 in R</span>
<span class='fu'><a href='sdf_len'>sdf_len</a></span>(<span class='no'>sc</span>, <span class='fl'>10</span>) <span class='kw'>%&gt;%</span> <span class='fu'>spark_apply</span>(<span class='kw'>function</span>(<span class='no'>df</span>) <span class='no'>df</span> * <span class='fl'>10</span>)

<span class='co'># }</span><div class='input'>
</div></code></pre></div>
    </div>

    </div>

