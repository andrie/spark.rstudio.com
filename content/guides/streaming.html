---
title: "R interface for Spark Streaming"
output:
  html_document:
    fig_width: 9
    fig_height: 5
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>As stated in the Spark’s official site, <a href="(https://spark.apache.org/streaming/)">Spark Streaming</a> makes it easy to build scalable fault-tolerant streaming applications. Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. Please see <a href="https://spark.apache.org/docs/2.1.3/structured-streaming-programming-guide.html">Spark’s official documentation</a> for a deeper look into Spark Streaming.</p>
</div>
<div id="scope-of-the-sparklyr-interface" class="section level2">
<h2>Scope of the <code>sparklyr</code> interface</h2>
<p>The <code>sparklyr</code> interface provides the following:</p>
<ul>
<li>Ability to run <a href="/dplyr">dplyr</a>, SQL, <a href="/guides/distributed-r/">spark_apply()</a>, and <a href="/guides/pipelines/#introduction-to-ml-pipelines">PipelineModels</a> against a stream</li>
<li>Read and write streams in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc</li>
<li>An out-of-the box graph visualization to monitor the stream</li>
<li>A new <code>reactiveSpark()</code> function, that allows Shiny apps to poll the contents of the stream
create Shiny apps that are able to read the contents of the stream</li>
</ul>
</div>
<div id="interacting-with-a-stream" class="section level2">
<h2>Interacting with a stream</h2>
<p>A good way of looking at the way how Spark streams update is as a three stage operation:</p>
<ol style="list-style-type: decimal">
<li><strong>Input</strong> - Spark reads the data inside a given folder. The folder is expected to contain multiple data files, with new files being created containing the most current stream data.</li>
<li><strong>Processing</strong> - Spark applies the desired operations on top of the data. These operations could be data manipulations (<code>dplyr</code>, SQL), data transformations (<code>sdf</code> operations, PipelineModel predictions), or native R manipulations (<code>spark_apply()</code>).</li>
<li><strong>Output</strong> - The results of processing the input files are saved in a different folder.</li>
</ol>
<p>In the same way all of the read and write operations in <code>sparklyr</code> for Spark Standalone, or in <code>sparklyr</code>’s local mode, the input and output folders are actual OS file system folders. For Hadoop clusters, these will be folder locations inside the HDFS.</p>
</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>Here is a small script that can be used with a local master. The result should be to see the <code>stream_view()</code> app showing live the number of records processed for each iteration of test data being sent to the stream. After the 50 iterations are complete, you can run the second code chunk to stop the job and close the Spark connection:</p>
<pre class="r"><code>library(dplyr)
library(future)
library(sparklyr)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)

read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

write_output &lt;- stream_write_csv(read_folder, &quot;source-out&quot;)

invisible(future(stream_generate_test(interval = 0.5)))

stream_view(write_output)</code></pre>
<pre class="r"><code>stream_stop(write_output)
spark_disconnect(sc)</code></pre>
<div id="code-breakdown" class="section level3">
<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
<li><p>Open the Spark connection</p>
<pre class="r"><code>library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)</code></pre></li>
<li><p>Optional step. This resets the input and output folders. It makes it easier to run the code multiple times in a clean manner.</p>
<pre class="r"><code>if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)</code></pre></li>
<li><p>Produces a single test file inside the “source” folder. This allows the “read” function to infer CSV file definition.</p>
<pre class="r"><code>stream_generate_test(iterations = 1)
list.files(&quot;source&quot;)</code></pre>
<pre><code>[1] &quot;stream_1.csv&quot;</code></pre></li>
<li><p>Points the stream reader to the folder where the streaming files will be placed. Since it is primed with a single CSV file, it will use as the expected layout of subsequent files.</p>
<pre class="r"><code>read_folder &lt;- stream_read_csv(sc, &quot;source&quot;)</code></pre></li>
<li><p><strong>The output writer is what starts the streaming job</strong>. It will start monitoring the input folder, and then write the new results in the “source-out” folder. So as new records stream in, new files will be created in the “source-out” folder. Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.</p>
<pre class="r"><code>write_output &lt;- stream_write_csv(read_folder, &quot;source-out&quot;)
list.files(&quot;source-out&quot;)</code></pre>
<pre><code>[1] &quot;_spark_metadata&quot;                                     &quot;checkpoint&quot;
[3] &quot;part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv&quot;</code></pre></li>
<li><p>The test generation function defaults to 50 iterations. In this case, the <code>interval</code> is set to half a second, meaning that the streaming will last for 25 seconds. To run the tests “out-of-sync” with the current R session, in this example the test generator will run using the <code>future</code> package.</p>
<pre class="r"><code>library(future)
invisible(future(stream_generate_test(interval = 0.5)))</code></pre></li>
<li><p>The <code>stream_view()</code> function can be used before the 50 tests are complete because of the use of the <code>future</code> package. It will monitor the status of the job that <code>write_output</code> is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.</p>
<pre class="r"><code>stream_view(write_output)</code></pre></li>
<li><p>The monitor will continue to run even after the tests are complete. To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.</p>
<pre class="r"><code>stream_stop(write_output)
spark_disconnect(sc)</code></pre></li>
</ol>
</div>
</div>
